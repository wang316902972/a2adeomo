"""
A2A Framework Demo: CrewAI SQL ä¼˜åŒ–ç³»ç»Ÿ (å• Agent æ¶æ„)
æ•´åˆ CrewAI (å•ä¸€ç»¼åˆAgent) è¿›è¡Œ SQL ä¼˜åŒ–åˆ†æ

å®‰è£…ä¾èµ–:
pip install crewai crewai-tools python-dotenv

ç¯å¢ƒå˜é‡é…ç½® (.env):
OPENAI_API_KEY=your_api_key_here
OPENAI_BASE_URL=your_ollama_url_here

æ¶æ„ç‰¹ç‚¹:
- å•ä¸€ç»¼åˆ SQL ä¸“å®¶ Agent ä»£æ›¿å¤š Agent åä½œ
- é›†æˆåˆ†æã€ä¼˜åŒ–ã€æŠ¥å‘Šç”Ÿæˆäºä¸€ä½“
- ç®€åŒ–å·¥ä½œæµç¨‹ï¼Œæé«˜æ‰§è¡Œæ•ˆç‡
"""

import json
import os
import re
import hashlib
import time
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âš ï¸  dotenv æœªå®‰è£…ï¼Œè·³è¿‡ .env æ–‡ä»¶åŠ è½½")

# CrewAI imports
from crewai import Agent, Task, Crew, Process
from crewai.tools import tool

os.environ["OPENAI_BASE_URL"] = "http://192.168.244.189:11434/v1"
os.environ["OPENAI_API_KEY"] = "ollama"

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ============================================================================
# é«˜æ€§èƒ½ SQL åˆ†æå¼•æ“
# ============================================================================

@dataclass
class SQLAnalysisResult:
    """SQLåˆ†æç»“æœæ•°æ®ç»“æ„"""
    issues: List[str]
    suggestions: List[str]
    metrics: Dict[str, Any]
    processing_time: float

class SQLAnalyzer:
    """é«˜æ€§èƒ½SQLåˆ†æå™¨ - æ›¿ä»£ç®€å•çš„å·¥å…·å‡½æ•°"""

    # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    PATTERNS = {
        'select_star': re.compile(r'\bSELECT\s+\*', re.IGNORECASE),
        'missing_where': re.compile(r'\bSELECT\b.+\bFROM\b(?!\s*\w+\s+WHERE)', re.IGNORECASE | re.DOTALL),
        'join_count': re.compile(r'\bJOIN\b', re.IGNORECASE),
        'or_condition': re.compile(r'\bWHERE\b.*\bOR\b', re.IGNORECASE),
        'like_wildcard': re.compile(r'\bLIKE\b\s*[\'"]\s*%'),
        'subquery': re.compile(r'\bSELECT\b.*\bSELECT\b', re.IGNORECASE | re.DOTALL),
        'distinct': re.compile(r'\bDISTINCT\b', re.IGNORECASE),
        'order_by': re.compile(r'\bORDER\s+BY\b', re.IGNORECASE),
        'insert_into': re.compile(r'\bINSERT\s+INTO\b', re.IGNORECASE),
        'group_by': re.compile(r'\bGROUP\s+BY\b', re.IGNORECASE),
        'index_hint': re.compile(r'\bUSE\s+INDEX\b|\bFORCE\s+INDEX\b', re.IGNORECASE)
    }

    def __init__(self):
        self.cache = {}  # ç®€å•å†…å­˜ç¼“å­˜
        self.hit_count = 0
        self.miss_count = 0

    def _get_sql_hash(self, sql_query: str) -> str:
        """ç”ŸæˆSQLæŸ¥è¯¢çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        normalized_sql = re.sub(r'\s+', ' ', sql_query.strip())
        return hashlib.md5(normalized_sql.encode()).hexdigest()

    @lru_cache(maxsize=128)
    def _cached_pattern_analysis(self, sql_hash: str, patterns_key: str) -> Tuple:
        """ç¼“å­˜æ¨¡å¼åˆ†æç»“æœ"""
        return ()

    def analyze_fast(self, sql_query: str) -> SQLAnalysisResult:
        """å¿«é€ŸSQLåˆ†æ - ä¼˜åŒ–ç‰ˆæœ¬"""
        start_time = time.time()

        # æ£€æŸ¥ç¼“å­˜
        sql_hash = self._get_sql_hash(sql_query)
        if sql_hash in self.cache:
            self.hit_count += 1
            cached_result = self.cache[sql_hash]
            cached_result.processing_time = time.time() - start_time
            return cached_result

        self.miss_count += 1

        # å¹¶è¡Œåˆ†æå¤šä¸ªæ¨¡å¼
        issues = []
        suggestions = []
        metrics = {
            'joins': 0,
            'subqueries': 0,
            'select_star': False,
            'missing_where': False,
            'has_index_hint': False
        }

        with ThreadPoolExecutor(max_workers=4) as executor:
            # æäº¤å¹¶è¡Œåˆ†æä»»åŠ¡
            futures = {
                executor.submit(self._analyze_pattern, sql_query, pattern_name): pattern_name
                for pattern_name in self.PATTERNS.keys()
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                pattern_name = futures[future]
                try:
                    result = future.result(timeout=0.1)  # å¿«é€Ÿè¶…æ—¶
                    if result:
                        issues.extend(result.get('issues', []))
                        suggestions.extend(result.get('suggestions', []))
                        if 'metrics' in result:
                            metrics.update(result['metrics'])
                except Exception:
                    continue  # å¿½ç•¥å•ä¸ªæ¨¡å¼åˆ†æå¤±è´¥

        # ç¼“å­˜ç»“æœ
        analysis_result = SQLAnalysisResult(
            issues=list(set(issues)),  # å»é‡
            suggestions=list(set(suggestions)),  # å»é‡
            metrics=metrics,
            processing_time=time.time() - start_time
        )

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.cache) > 100:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache[sql_hash] = analysis_result
        return analysis_result

    def _analyze_pattern(self, sql_query: str, pattern_name: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ¨¡å¼"""
        sql_lower = sql_query.lower()
        pattern = self.PATTERNS[pattern_name]

        result = {'issues': [], 'suggestions': [], 'metrics': {}}

        if pattern_name == 'select_star' and pattern.search(sql_query):
            result['issues'].append("âŒ ä½¿ç”¨ SELECT * ä¼šæ£€ç´¢æ‰€æœ‰åˆ—ï¼Œå»ºè®®æ˜ç¡®æŒ‡å®šéœ€è¦çš„åˆ—")
            result['suggestions'].append("æ˜ç¡®åˆ—åä¼˜åŒ–: åªé€‰æ‹©éœ€è¦çš„åˆ—ä»¥å‡å°‘æ•°æ®ä¼ è¾“")
            result['metrics']['select_star'] = True

        elif pattern_name == 'missing_where':
            # æ›´ç²¾ç¡®çš„WHEREå­å¥æ£€æµ‹
            if pattern.search(sql_query) and not re.search(r'\bINSERT\s+INTO\b', sql_query, re.IGNORECASE):
                result['issues'].append("âŒ ç¼ºå°‘ WHERE å­å¥å¯èƒ½å¯¼è‡´å…¨è¡¨æ‰«æ")
                result['suggestions'].append("æ·»åŠ è¿‡æ»¤æ¡ä»¶: ä½¿ç”¨WHEREå­å¥é™åˆ¶æ‰«æèŒƒå›´")
                result['metrics']['missing_where'] = True

        elif pattern_name == 'join_count':
            joins = pattern.findall(sql_query)
            if len(joins) > 3:
                result['issues'].append(f"âš ï¸  å‘ç° {len(joins)} ä¸ª JOINï¼Œå¯èƒ½å½±å“æ€§èƒ½")
                result['suggestions'].append("ä¼˜åŒ–å¤šè¡¨å…³è”: è€ƒè™‘ä½¿ç”¨CTEæˆ–åˆ†è§£å¤æ‚æŸ¥è¯¢")
            result['metrics']['joins'] = len(joins)

        elif pattern_name == 'or_condition' and pattern.search(sql_query):
            result['issues'].append("âš ï¸  OR æ¡ä»¶å¯èƒ½æ— æ³•æœ‰æ•ˆä½¿ç”¨ç´¢å¼•")
            result['suggestions'].append("ORæ¡ä»¶ä¼˜åŒ–: è€ƒè™‘ä½¿ç”¨UNIONæˆ–INå­å¥æ›¿ä»£")

        elif pattern_name == 'like_wildcard' and pattern.search(sql_query):
            result['issues'].append("âŒ LIKE å‰ç½®é€šé…ç¬¦æ— æ³•ä½¿ç”¨ç´¢å¼•")
            result['suggestions'].append("æ¨¡ç³ŠæŸ¥è¯¢ä¼˜åŒ–: æ”¹ä¸ºåç½®é€šé…ç¬¦æˆ–ä½¿ç”¨å…¨æ–‡æœç´¢")

        elif pattern_name == 'subquery':
            subqueries = pattern.findall(sql_query)
            if len(subqueries) > 1:
                result['issues'].append("ğŸ’¡ å­˜åœ¨å¤šä¸ªå­æŸ¥è¯¢ï¼Œè€ƒè™‘æ˜¯å¦å¯ä»¥ç”¨ JOIN ä¼˜åŒ–")
                result['suggestions'].append("å­æŸ¥è¯¢ä¼˜åŒ–: è€ƒè™‘å°†ç›¸å…³å­æŸ¥è¯¢æ”¹ä¸ºJOIN")
            result['metrics']['subqueries'] = len(subqueries) - 1

        elif pattern_name == 'distinct' and pattern.search(sql_query):
            result['issues'].append("ğŸ’¡ ä½¿ç”¨ DISTINCT å¯èƒ½å½±å“æ€§èƒ½")
            result['suggestions'].append("DISTINCTä¼˜åŒ–: æ£€æŸ¥æ˜¯å¦å¿…è¦ï¼Œæˆ–ä½¿ç”¨GROUP BYæ›¿ä»£")

        elif pattern_name == 'order_by' and pattern.search(sql_query):
            result['issues'].append("ğŸ’¡ ORDER BY æ“ä½œéœ€è¦æ’åºï¼Œç¡®ä¿ç›¸å…³åˆ—æœ‰ç´¢å¼•")
            result['suggestions'].append("æ’åºä¼˜åŒ–: ç¡®ä¿ORDER BYåˆ—æœ‰é€‚å½“ç´¢å¼•")

        elif pattern_name == 'index_hint' and pattern.search(sql_query):
            result['metrics']['has_index_hint'] = True

        return result

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total_requests * 100) if total_requests > 0 else 0
        return {
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': f"{hit_rate:.1f}%",
            'cache_size': len(self.cache)
        }

# å…¨å±€åˆ†æå™¨å®ä¾‹
sql_analyzer = SQLAnalyzer()

# ============================================================================
# 1. CrewAI SQL ä¼˜åŒ– Agent (å®Œæ•´å®ç°)
# ============================================================================

@tool("SQL Analysis Tool")
def analyze_sql_tool(sql_query: str) -> str:
    """é«˜æ€§èƒ½ SQL è¯­å¥åˆ†æå·¥å…·ï¼Œè¯†åˆ«æ€§èƒ½é—®é¢˜å’Œä¼˜åŒ–æœºä¼š

    Args:
        sql_query: è¦åˆ†æçš„ SQL æŸ¥è¯¢è¯­å¥

    Returns:
        åˆ†æç»“æœå­—ç¬¦ä¸²ï¼ŒåŒ…å«å‘ç°çš„é—®é¢˜å’Œå»ºè®®
    """
    # ä½¿ç”¨é«˜æ€§èƒ½åˆ†æå™¨
    analysis_result = sql_analyzer.analyze_fast(sql_query)

    if not analysis_result.issues:
        return f"âœ… SQL è¯­å¥çœ‹èµ·æ¥ä¸é”™ï¼Œæ²¡æœ‰æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜ (åˆ†æè€—æ—¶: {analysis_result.processing_time:.3f}s)"

    issues_text = "å‘ç°ä»¥ä¸‹é—®é¢˜:\n" + "\n".join(analysis_result.issues)

    # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
    if analysis_result.metrics:
        metrics_summary = f"\n\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:\n"
        if analysis_result.metrics.get('joins', 0) > 0:
            metrics_summary += f"   â€¢ JOIN æ•°é‡: {analysis_result.metrics['joins']}\n"
        if analysis_result.metrics.get('subqueries', 0) > 0:
            metrics_summary += f"   â€¢ å­æŸ¥è¯¢æ•°é‡: {analysis_result.metrics['subqueries']}\n"
        if analysis_result.metrics.get('select_star', False):
            metrics_summary += f"   â€¢ ä½¿ç”¨äº† SELECT *\n"
        if analysis_result.metrics.get('missing_where', False):
            metrics_summary += f"   â€¢ ç¼ºå°‘ WHERE å­å¥\n"

        issues_text += metrics_summary

    issues_text += f"\n\nâš¡ åˆ†æè€—æ—¶: {analysis_result.processing_time:.3f}s"
    return issues_text

@tool("SQL Optimization Tool")
def generate_optimization_suggestions(sql_query: str) -> str:
    """æ ¹æ® SQL åˆ†æç»“æœç”Ÿæˆå…·ä½“çš„ä¼˜åŒ–å»ºè®®

    Args:
        sql_query: è¦ä¼˜åŒ–çš„ SQL æŸ¥è¯¢è¯­å¥

    Returns:
        ä¼˜åŒ–å»ºè®®å­—ç¬¦ä¸²
    """
    # ä½¿ç”¨é«˜æ€§èƒ½åˆ†æå™¨è·å–åˆ†æç»“æœ
    analysis_result = sql_analyzer.analyze_fast(sql_query)

    if not analysis_result.suggestions:
        return "å½“å‰ SQL å·²ç»è¾ƒä¸ºä¼˜åŒ–ï¼Œå»ºè®®:\n1. ç¡®ä¿ç›¸å…³åˆ—æœ‰ç´¢å¼•\n2. ä½¿ç”¨ EXPLAIN åˆ†ææ‰§è¡Œè®¡åˆ’\n3. ç›‘æ§å®é™…æ‰§è¡Œæ€§èƒ½"

    suggestions = []

    # æ ¹æ®åˆ†æç»“æœç”Ÿæˆè¯¦ç»†å»ºè®®
    metrics = analysis_result.metrics

    if metrics.get('select_star', False):
        suggestions.append(f"""
ä¼˜åŒ–å»ºè®® 1: æ˜ç¡®åˆ—å
- é—®é¢˜: SELECT * æ£€ç´¢æ‰€æœ‰åˆ—ï¼Œå¢åŠ ç½‘ç»œä¼ è¾“å’Œå†…å­˜æ¶ˆè€—
- æ–¹æ¡ˆ: åªé€‰æ‹©ä¸šåŠ¡éœ€è¦çš„åˆ—
- ç¤ºä¾‹: SELECT id, name, email, created_at FROM users WHERE status = 'active'
- é¢„æœŸæ”¶ç›Š: å‡å°‘30-70%æ•°æ®ä¼ è¾“é‡ï¼Œæå‡æŸ¥è¯¢é€Ÿåº¦
        """)

    if metrics.get('missing_where', False):
        suggestions.append(f"""
ä¼˜åŒ–å»ºè®® 2: æ·»åŠ è¿‡æ»¤æ¡ä»¶
- é—®é¢˜: ç¼ºå°‘ WHERE å­å¥å¯¼è‡´å…¨è¡¨æ‰«æ
- æ–¹æ¡ˆ: æ·»åŠ æ—¶é—´èŒƒå›´ã€çŠ¶æ€é™åˆ¶ç­‰è¿‡æ»¤æ¡ä»¶
- ç¤ºä¾‹: WHERE created_at >= DATE_SUB(NOW(), INTERVAL 30 DAY) AND status = 'active'
- é¢„æœŸæ”¶ç›Š: å‡å°‘90%+æ‰«æè¡Œæ•°ï¼Œé¿å…å…¨è¡¨é”å®š
        """)

    if analysis_result.issues:
        for issue in analysis_result.issues:
            if "JOIN" in issue:
                suggestions.append(f"""
ä¼˜åŒ–å»ºè®® 3: ä¼˜åŒ–å¤šè¡¨å…³è”
- é—®é¢˜: {analysis_result.metrics.get('joins', 0)} ä¸ª JOIN æ“ä½œå¯èƒ½å¯¼è‡´ç¬›å¡å°”ç§¯
- æ–¹æ¡ˆ:
  * ä½¿ç”¨è¦†ç›–ç´¢å¼•ä¼˜åŒ–è¿æ¥æ¡ä»¶
  * è€ƒè™‘ä½¿ç”¨ CTE åˆ†æ­¥å¤„ç†å¤æ‚å…³è”
  * æ·»åŠ é€‚å½“çš„è¿‡æ»¤æ¡ä»¶å‡å°‘è¿æ¥æ•°æ®é‡
- ç¤ºä¾‹:
  WITH filtered_data AS (
    SELECT user_id, COUNT(*) as order_count
    FROM orders
    WHERE created_at >= '2024-01-01' AND status = 'completed'
    GROUP BY user_id
  )
  SELECT u.*, fd.order_count FROM users u
  INNER JOIN filtered_data fd ON u.id = fd.user_id
- é¢„æœŸæ”¶ç›Š: å‡å°‘50-80%çš„è¿æ¥è®¡ç®—å¼€é”€
                """)
                break

    for issue in analysis_result.issues:
        if "OR" in issue:
            suggestions.append(f"""
ä¼˜åŒ–å»ºè®® 4: ä¼˜åŒ– OR æ¡ä»¶
- é—®é¢˜: OR æ¡ä»¶å¯èƒ½æ— æ³•æœ‰æ•ˆä½¿ç”¨ç´¢å¼•
- æ–¹æ¡ˆ:
  * ä½¿ç”¨ UNION æ›¿ä»£ OR (é€‚ç”¨äºä¸åŒåˆ—)
  * ä½¿ç”¨ IN å­å¥æ›¿ä»£ OR (é€‚ç”¨äºåŒåˆ—)
  * è€ƒè™‘ä½¿ç”¨å¤åˆç´¢å¼•è¦†ç›– OR æ¡ä»¶
- ç¤ºä¾‹: SELECT * FROM users WHERE status IN ('active', 'pending')
- é¢„æœŸæ”¶ç›Š: æå‡20-60%æŸ¥è¯¢æ€§èƒ½
            """)
            break

    for issue in analysis_result.issues:
        if "LIKE" in issue and "%" in issue:
            suggestions.append(f"""
ä¼˜åŒ–å»ºè®® 5: ä¼˜åŒ–æ¨¡ç³ŠæŸ¥è¯¢
- é—®é¢˜: å‰ç½®é€šé…ç¬¦å¯¼è‡´å…¨è¡¨æ‰«æ
- æ–¹æ¡ˆ:
  * ä½¿ç”¨åç½®é€šé…ç¬¦: LIKE 'keyword%'
  * ä½¿ç”¨å…¨æ–‡ç´¢å¼•: MATCH(title) AGAINST('keyword' IN NATURAL LANGUAGE MODE)
  * ä½¿ç”¨å¤–éƒ¨æœç´¢å¼•æ“: Elasticsearch/Solr
- é¢„æœŸæ”¶ç›Š: æå‡10-100å€æœç´¢æ€§èƒ½
            """)
            break

    # å¦‚æœæ²¡æœ‰ç”Ÿæˆå…·ä½“å»ºè®®ï¼Œä½¿ç”¨é€šç”¨å»ºè®®
    if not suggestions:
        suggestions = [f"""
é€šç”¨ä¼˜åŒ–å»ºè®®:
- æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µï¼Œç¡®ä¿WHEREå’ŒJOINæ¡ä»¶åˆ—æœ‰åˆé€‚ç´¢å¼•
- ä½¿ç”¨EXPLAINåˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’
- è€ƒè™‘æŸ¥è¯¢ç»“æœçš„ç¼“å­˜ç­–ç•¥
- ç›‘æ§æŸ¥è¯¢æ‰§è¡Œæ—¶é—´å’Œèµ„æºæ¶ˆè€—
        """]

    result = "\n".join(suggestions)
    result += f"\n\nâš¡ åˆ†æå¼•æ“æ€§èƒ½: {analysis_result.processing_time:.3f}s"

    # æ·»åŠ ç¼“å­˜ç»Ÿè®¡
    cache_stats = sql_analyzer.get_cache_stats()
    result += f" | ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']}"

    return result


class SQLOptimizerSingle:
    """é«˜æ€§èƒ½å• Agent SQL ä¼˜åŒ–ç³»ç»Ÿ"""
    def __init__(self, openai_api_key: Optional[str] = None, use_fast_mode: bool = True):
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_BASE_URL")
        self.use_fast_mode = use_fast_mode  # å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡LLMï¼Œä½¿ç”¨æœ¬åœ°åˆ†æ

        if not self.api_key:
            raise ValueError("éœ€è¦è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")

        self._setup_llm()
        self._setup_agent()

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'fast_mode_hits': 0,
            'llm_mode_hits': 0,
            'total_processing_time': 0.0,
            'avg_processing_time': 0.0
        }

    def _setup_llm(self):
        """è®¾ç½® LLM é…ç½®"""
        try:
            # å°è¯•å¯¼å…¥ LLM
            from crewai import LLM

            # é…ç½® LLM
            self.llm = LLM(
                model="mistral:latest",  # ä½¿ç”¨OllamaæœåŠ¡å™¨ä¸Šçš„å®é™…æ¨¡å‹åç§°
                temperature=0.1,  # ä½æ¸©åº¦ä»¥ç¡®ä¿å‡†ç¡®æ€§
                api_key=self.api_key,
                base_url=self.base_url
            )
            print("âœ… LLM é…ç½®æˆåŠŸ")
        except ImportError:
            print("âš ï¸  æ— æ³•å¯¼å…¥ LLMï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.llm = None
        except Exception as e:
            print(f"âš ï¸  LLM é…ç½®å¤±è´¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {e}")
            self.llm = None

    def _setup_agent(self):
        """åˆå§‹åŒ–å•ä¸€ç»¼åˆ SQL Agent"""

        # ç»¼ä¸€çš„ agent é…ç½®å‚æ•°
        agent_config = {
            'verbose': True,
            'allow_delegation': False,
            'llm': self.llm
        }

        # å•ä¸€çš„ç»¼åˆ SQL ä¼˜åŒ–ä¸“å®¶
        self.sql_expert = Agent(
            role='SQL æ€§èƒ½ä¼˜åŒ–ä¸“å®¶',
            goal='ç»¼åˆåˆ†æ SQL è¯­å¥ï¼Œè¯†åˆ«æ€§èƒ½é—®é¢˜å¹¶æä¾›ä¼˜åŒ–æ–¹æ¡ˆï¼Œç”Ÿæˆæ¸…æ™°çš„ä¼˜åŒ–æŠ¥å‘Š',
            backstory="""ä½ æ˜¯ä¸€ä½æ‹¥æœ‰ 15 å¹´ç»éªŒçš„æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚

            **æ ¸å¿ƒèƒ½åŠ›ï¼š**
            1. **SQL åˆ†æ**: æ·±å…¥åˆ†æ SQL è¯­å¥ï¼Œè¯†åˆ«æ‰€æœ‰æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼š
            2. **ä¼˜åŒ–è®¾è®¡**: æ ¹æ®åˆ†æç»“æœï¼Œç”Ÿæˆä¼˜åŒ–çš„ SQL è¯­å¥å’Œè¯¦ç»†çš„ä¼˜åŒ–æ–¹æ¡ˆ
            3. **æ–‡æ¡£æ’°å†™**: å°†æŠ€æœ¯åˆ†æç»“æœè½¬åŒ–ä¸ºæ¸…æ™°ã€ä¸“ä¸šçš„ä¼˜åŒ–æŠ¥å‘Š
            4. **æœ€ä½³å®è·µ**: æä¾›ç¬¦åˆä¸šç•Œæ ‡å‡†çš„ SQL ä¼˜åŒ–å»ºè®®å’Œå®æ–½æŒ‡å¯¼

            **ä¸“é•¿é¢†åŸŸï¼š**
            - MySQLã€PostgreSQLã€Oracle ç­‰ä¸»æµæ•°æ®åº“ä¼˜åŒ–
            - å¤æ‚æŸ¥è¯¢é‡æ„å’Œæ€§èƒ½è°ƒä¼˜
            - ç´¢å¼•è®¾è®¡å’ŒæŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–
            - æ•°æ®åº“æ¶æ„å»ºè®®

            **å·¥ä½œæµç¨‹ï¼š**
            - æ¥æ”¶ SQL è¾“å…¥
            - å…¨é¢åˆ†ææ€§èƒ½é—®é¢˜
            - è®¾è®¡ä¼˜åŒ–ç­–ç•¥
            - ç”Ÿæˆä¼˜åŒ–åçš„ SQL
            - æä¾›è¯¦ç»†çš„ä¼˜åŒ–æŠ¥å‘Š

            ä½ çš„åˆ†ææ€»æ˜¯å…¨é¢ã€å‡†ç¡®ã€æœ‰ç†æœ‰æ®ï¼Œä¼˜åŒ–æ–¹æ¡ˆå…¼é¡¾æ€§èƒ½å’Œå¯è¯»æ€§ã€‚""",
            tools=[analyze_sql_tool, generate_optimization_suggestions],
            **agent_config
        )
    
    def _fast_optimize(self, sql_query: str) -> Dict[str, Any]:
        """å¿«é€Ÿä¼˜åŒ–æ¨¡å¼ - ç›´æ¥ä½¿ç”¨é«˜æ€§èƒ½åˆ†æå™¨ï¼Œæ— éœ€LLM"""
        start_time = time.time()

        # ä½¿ç”¨é«˜æ€§èƒ½åˆ†æå™¨
        analysis_result = sql_analyzer.analyze_fast(sql_query)

        # ç”Ÿæˆä¼˜åŒ–åçš„SQL
        optimized_sql = self._apply_fast_optimizations(sql_query, analysis_result)

        # è®¡ç®—æ€§èƒ½æå‡ä¼°ç®—
        performance_gain = self._estimate_performance_gain(analysis_result)

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(analysis_result)

        processing_time = time.time() - start_time

        return {
            "original_sql": sql_query,
            "optimized_sql": optimized_sql,
            "issues_found": analysis_result.issues,
            "optimizations_applied": analysis_result.suggestions,
            "performance_gain_estimate": performance_gain,
            "recommendations": recommendations,
            "processing_mode": "fast",
            "processing_time": processing_time,
            "analysis_metrics": analysis_result.metrics
        }

    def _apply_fast_optimizations(self, sql_query: str, analysis_result: SQLAnalysisResult) -> str:
        """åº”ç”¨å¿«é€Ÿä¼˜åŒ–è§„åˆ™"""
        optimized = sql_query

        # å¦‚æœSELECT *ï¼Œä¼˜åŒ–ä¸ºå…·ä½“åˆ—ï¼ˆéœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­ï¼‰
        if analysis_result.metrics.get('select_star', False):
            # ç®€å•çš„å¯å‘å¼ä¼˜åŒ–ï¼šå¦‚æœæœ‰è¡¨åï¼Œå‡è®¾ä¸»é”®åˆ—
            table_match = re.search(r'FROM\s+(\w+)', sql_query, re.IGNORECASE)
            if table_match:
                table_name = table_match.group(1)
                optimized = re.sub(
                    r'SELECT\s+\*',
                    f'SELECT id, name, created_at',  # é€šç”¨åˆ—å
                    optimized,
                    flags=re.IGNORECASE
                )

        # å¦‚æœæ²¡æœ‰WHEREä¸”æ˜¯SELECTæŸ¥è¯¢ï¼Œæ·»åŠ åŸºæœ¬è¿‡æ»¤
        if analysis_result.metrics.get('missing_where', False):
            # ä¸ºINSERTæŸ¥è¯¢è·³è¿‡WHEREä¼˜åŒ–
            if not re.search(r'\bINSERT\s+INTO\b', optimized, re.IGNORECASE):
                table_match = re.search(r'FROM\s+(\w+)', optimized, re.IGNORECASE)
                if table_match:
                    # åœ¨GROUP BYä¹‹å‰æ·»åŠ WHERE
                    optimized = re.sub(
                        r'(GROUP\s+BY)',
                        'WHERE status = \'active\' AND created_at >= DATE_SUB(NOW(), INTERVAL 30 DAY) \\1',
                        optimized,
                        flags=re.IGNORECASE
                    )

        return optimized

    def _estimate_performance_gain(self, analysis_result: SQLAnalysisResult) -> str:
        """ä¼°ç®—æ€§èƒ½æå‡"""
        gains = []

        if analysis_result.metrics.get('select_star', False):
            gains.append("30-50%")

        if analysis_result.metrics.get('missing_where', False):
            gains.append("70-90%")

        if analysis_result.metrics.get('joins', 0) > 3:
            gains.append("40-60%")

        if any("LIKE" in issue for issue in analysis_result.issues):
            gains.append("10-100å€")

        if not gains:
            return "5-15%"

        # å–æœ€ä½å’Œæœ€é«˜ä¼°ç®—
        return f"{min(gains)} - {max(gains)}"

    def _generate_recommendations(self, analysis_result: SQLAnalysisResult) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if analysis_result.metrics.get('select_star', False):
            recommendations.append("æ˜ç¡®æŒ‡å®šæŸ¥è¯¢åˆ—ï¼Œé¿å…SELECT *")

        if analysis_result.metrics.get('missing_where', False):
            recommendations.append("æ·»åŠ é€‚å½“çš„WHEREæ¡ä»¶é™åˆ¶æ‰«æèŒƒå›´")

        if analysis_result.metrics.get('joins', 0) > 3:
            recommendations.append("è€ƒè™‘ä½¿ç”¨CTEæˆ–åˆ†è§£å¤æ‚JOINæ“ä½œ")

        recommendations.extend([
            "ä½¿ç”¨EXPLAINåˆ†æå®é™…æ‰§è¡Œè®¡åˆ’",
            "ç¡®ä¿WHEREå’ŒJOINæ¡ä»¶åˆ—æœ‰é€‚å½“ç´¢å¼•",
            "ç›‘æ§æŸ¥è¯¢æ‰§è¡Œæ€§èƒ½"
        ])

        return list(set(recommendations))  # å»é‡

    def optimize_sql(self, sql_query: str, force_llm: bool = False) -> Dict[str, Any]:
        """æ‰§è¡Œ SQL ä¼˜åŒ–æµç¨‹ - ä¼˜åŒ–ç‰ˆæœ¬"""
        start_time = time.time()
        self.stats['total_requests'] += 1

        print("\n" + "="*80)
        print("ğŸš€ é«˜æ€§èƒ½ SQL ä¼˜åŒ–æµç¨‹å¯åŠ¨")
        print("="*80)

        # å¿«é€Ÿæ¨¡å¼å†³ç­–
        if self.use_fast_mode and not force_llm:
            print("âš¡ ä½¿ç”¨å¿«é€Ÿä¼˜åŒ–æ¨¡å¼ (æœ¬åœ°åˆ†æå¼•æ“)")
            self.stats['fast_mode_hits'] += 1
            result = self._fast_optimize(sql_query)

            # æ·»åŠ å…ƒæ•°æ®
            result.update({
                "timestamp": datetime.now().isoformat(),
                "agent": "fast_sql_optimizer",
                "cache_stats": sql_analyzer.get_cache_stats()
            })

            processing_time = time.time() - start_time
            self.stats['total_processing_time'] += processing_time
            self.stats['avg_processing_time'] = self.stats['total_processing_time'] / self.stats['total_requests']

            print(f"âœ… å¿«é€Ÿä¼˜åŒ–å®Œæˆ (è€—æ—¶: {processing_time:.3f}s)")
            return result

        # LLMæ¨¡å¼ - åŸæœ‰é€»è¾‘ä¼˜åŒ–
        print("ğŸ§  ä½¿ç”¨ CrewAI æ·±åº¦åˆ†ææ¨¡å¼")
        self.stats['llm_mode_hits'] += 1

        # å•ä¸€ç»¼åˆä»»åŠ¡ï¼šå®Œæ•´çš„ SQL ä¼˜åŒ–åˆ†æ
        comprehensive_task = Task(
            description=f"""
            è¯·å¯¹ä»¥ä¸‹ SQL è¯­å¥è¿›è¡Œå®Œæ•´çš„æ€§èƒ½ä¼˜åŒ–åˆ†æ:

            ```sql
            {sql_query}
            ```

            è¯·ä½¿ç”¨æä¾›çš„å·¥å…·å®Œæˆä»¥ä¸‹å…¨æµç¨‹åˆ†æ:

            **ç¬¬ä¸€é˜¶æ®µ: SQL åˆ†æ**
            - ä½¿ç”¨ SQL Analysis Tool åˆ†æè¯­å¥ä¸­çš„æ€§èƒ½é—®é¢˜
            - è¯†åˆ«ç´¢å¼•ä½¿ç”¨æƒ…å†µã€æŸ¥è¯¢æ•ˆç‡ã€æ½œåœ¨ç“¶é¢ˆ
            - åˆ—å‡ºæ‰€æœ‰å‘ç°çš„é—®é¢˜å¹¶æ ‡æ³¨ä¸¥é‡ç¨‹åº¦

            **ç¬¬äºŒé˜¶æ®µ: ä¼˜åŒ–è®¾è®¡**
            - ä½¿ç”¨ SQL Optimization Tool ç”Ÿæˆå…·ä½“çš„ä¼˜åŒ–å»ºè®®
            - è®¾è®¡ä¼˜åŒ–åçš„ SQL è¯­å¥
            - è¯„ä¼°é¢„æœŸçš„æ€§èƒ½æå‡å’Œå®æ–½æ³¨æ„äº‹é¡¹

            **ç¬¬ä¸‰é˜¶æ®µ: æŠ¥å‘Šç”Ÿæˆ**
            æ•´åˆæ‰€æœ‰åˆ†æç»“æœï¼Œç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„å®Œæ•´æŠ¥å‘Š:
            1. åŸå§‹ SQL å’Œä¼˜åŒ–åçš„ SQL å¯¹æ¯”
            2. å‘ç°çš„é—®é¢˜åˆ—è¡¨
            3. ä¼˜åŒ–æªæ–½è¯¦è§£
            4. é¢„æœŸæ€§èƒ½æå‡
            5. å®æ–½å»ºè®®

            **è¾“å‡ºæ ¼å¼è¦æ±‚:**
            - ä½¿ç”¨ JSON æ ¼å¼è¾“å‡ºæœ€ç»ˆç»“æœ
            - ç»“æ„æ¸…æ™°ï¼Œæ˜“äºè§£æ
            - åŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯

            **JSON ç»“æ„ç¤ºä¾‹:**
            {{
                "original_sql": "åŸå§‹ SQL",
                "optimized_sql": "ä¼˜åŒ–åçš„ SQL",
                "issues_found": ["é—®é¢˜1", "é—®é¢˜2"],
                "optimizations_applied": ["ä¼˜åŒ–1", "ä¼˜åŒ–2"],
                "performance_gain_estimate": "é¢„ä¼°æå‡ç™¾åˆ†æ¯”",
                "recommendations": ["å»ºè®®1", "å»ºè®®2"]
            }}

            **å·¥ä½œåŸåˆ™:**
            - ä¿æŒ SQL è¯­ä¹‰ä¸å˜
            - ä¼˜å…ˆè€ƒè™‘æ€§èƒ½æå‡
            - å…¼é¡¾ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
            - æä¾›ç¬¦åˆä¸šç•Œæ ‡å‡†çš„ä¼˜åŒ–å»ºè®®
            """,
            agent=self.sql_expert,
            expected_output="JSON æ ¼å¼çš„å®Œæ•´ SQL ä¼˜åŒ–æŠ¥å‘Šï¼ŒåŒ…å«åˆ†æã€ä¼˜åŒ–å’Œå»ºè®®"
        )

        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ LLM é…ç½®
        if not self.llm:
            print("âš ï¸ LLM é…ç½®å¤±è´¥ï¼Œåˆ‡æ¢åˆ°å¿«é€Ÿæ¨¡å¼")
            return self._fast_optimize(sql_query)

        # åˆ›å»º Crew å¹¶æ‰§è¡Œ (å• Agent æ¨¡å¼)
        crew = Crew(
            agents=[self.sql_expert],
            tasks=[comprehensive_task],
            process=Process.sequential,
            verbose=True
        )

        try:
            print("ğŸš€ å¼€å§‹æ‰§è¡Œ CrewAI ä»»åŠ¡...")
            # æ‰§è¡Œä»»åŠ¡
            result = crew.kickoff()
            print(f"ğŸ¯ CrewAI æ‰§è¡Œå®Œæˆï¼Œç»“æœç±»å‹: {type(result)}")

            # è§£æç»“æœ
            result_str = str(result)
            print(f"ğŸ“„ ç»“æœå­—ç¬¦ä¸²é•¿åº¦: {len(result_str)}")

            # å°è¯•æå– JSON
            json_start = result_str.find('{')
            json_end = result_str.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_str = result_str[json_start:json_end]
                print(f"ğŸ” æå–çš„ JSON é•¿åº¦: {len(json_str)}")
                parsed_result = json.loads(json_str)
                print("âœ… JSON è§£ææˆåŠŸ")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°å®Œæ•´ JSON")
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œåˆ›å»ºåŸºæœ¬ç»“æœ
                parsed_result = {
                    "original_sql": sql_query,
                    "optimized_sql": self._apply_fast_optimizations(sql_query, sql_analyzer.analyze_fast(sql_query)),
                    "issues_found": ["éœ€è¦è¯¦ç»†åˆ†æ"],
                    "optimizations_applied": ["åŸºç¡€ä¼˜åŒ–"],
                    "performance_gain_estimate": "10-20%",
                    "recommendations": ["ä½¿ç”¨ EXPLAIN åˆ†ææ‰§è¡Œè®¡åˆ’", "æ·»åŠ åˆé€‚çš„ç´¢å¼•"]
                }

        except Exception as e:
            print(f"âŒ CrewAI æ‰§è¡Œå‡ºé”™: {e}")
            print("ğŸ”„ ä½¿ç”¨å¿«é€Ÿä¼˜åŒ–é€»è¾‘")
            return self._fast_optimize(sql_query)

        # ç¡®ä¿åŸºæœ¬å­—æ®µå­˜åœ¨
        parsed_result = self._ensure_required_fields(parsed_result, sql_query)

        # æ·»åŠ å…ƒæ•°æ®
        processing_time = time.time() - start_time
        self.stats['total_processing_time'] += processing_time
        self.stats['avg_processing_time'] = self.stats['total_processing_time'] / self.stats['total_requests']

        parsed_result.update({
            "timestamp": datetime.now().isoformat(),
            "agent": "crewai_sql_optimizer",
            "processing_mode": "llm",
            "processing_time": processing_time,
            "cache_stats": sql_analyzer.get_cache_stats()
        })

        print(f"\nâœ… CrewAI ä¼˜åŒ–å®Œæˆ (è€—æ—¶: {processing_time:.3f}s)")
        return parsed_result

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            'fast_mode_ratio': f"{(self.stats['fast_mode_hits'] / max(self.stats['total_requests'], 1) * 100):.1f}%",
            'llm_mode_ratio': f"{(self.stats['llm_mode_hits'] / max(self.stats['total_requests'], 1) * 100):.1f}%"
        }
  



# ============================================================================

# ============================================================================
# 3. ç®€åŒ–ä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»ç¨‹åº - é«˜æ€§èƒ½ SQL ä¼˜åŒ–ç³»ç»Ÿæ¼”ç¤º"""

    # æ£€æŸ¥ API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("è¯·åˆ›å»º .env æ–‡ä»¶å¹¶æ·»åŠ : OPENAI_API_KEY=your_key_here")
        return

    try:
        print("\n" + "ğŸš€"*40)
        print("    é«˜æ€§èƒ½ SQL ä¼˜åŒ–ç³»ç»Ÿ v2.0")
        print("       (å¿«é€Ÿåˆ†æå¼•æ“ + CrewAI)")
        print("ğŸš€"*40)

        # åˆ›å»ºé«˜æ€§èƒ½ SQL ä¼˜åŒ–å™¨ (é»˜è®¤å¿«é€Ÿæ¨¡å¼)
        print("\nğŸ”§ åˆå§‹åŒ–é«˜æ€§èƒ½ SQL ä¼˜åŒ–å™¨...")
        sql_optimizer = SQLOptimizerSingle(use_fast_mode=True)

        # æµ‹è¯• SQL é›†åˆ
        test_queries = [
            {
                "name": "å¤æ‚ INSERT æŸ¥è¯¢",
                "sql": """
                INSERT INTO channeldiscount_mstradedailyanalyze_sjmy_tmp(TaskDate, ServerId, ChannelId, TotalLogNumber, AlarmLogNumber, TotalLogPrice, AlarmLogPrice, BaseAlarmNumberPercent, BaseAlarmPricePercent)
                        SELECT 251110, t01.ServerId, t03.ChannelId, COUNT(1) as TotalLogNumber, sum(CASE WHEN t01.AlarmStatus!=0 then 1 else 0 end) as AlarmLogNumber,
                        SUM(t01.TradeMoShi) as TotalLogPrice, sum(CASE WHEN t01.AlarmStatus!=0 then t01.TradeMoShi else 0 end) as AlarmLogPrice, t02.AlarmNumberPercent, t02.AlarmPricePercent
                        FROM channeldiscount_mstraderatioanalyze_sjmy t01
        inner join channeldiscount_serverchannel t03 on t03.GameId = 421 and t03.ServerId=t01.ServerId
        left join channeldiscount_mstradedailyconfig t02 on t02.GameId = 421 and t02.AlarmType=2
                        WHERE left(t01.TradeTime, 6) = 251110 AND t01.ServerId != 0
                        GROUP by ServerId;
                """
            },
            {
                "name": "SELECT * æŸ¥è¯¢",
                "sql": "SELECT * FROM users WHERE created_at > '2024-01-01' ORDER BY id DESC"
            },
            {
                "name": "å¤š JOIN æŸ¥è¯¢",
                "sql": """
                SELECT u.*, o.total_amount, p.product_name
                FROM users u
                JOIN orders o ON u.id = o.user_id
                JOIN order_items oi ON o.id = oi.order_id
                JOIN products p ON oi.product_id = p.id
                JOIN categories c ON p.category_id = c.id
                WHERE u.status = 'active'
                """
            }
        ]

        print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•å¼€å§‹ - å…± {len(test_queries)} ä¸ªæŸ¥è¯¢")
        print("="*80)

        total_start_time = time.time()

        for i, test_case in enumerate(test_queries, 1):
            print(f"\nã€æµ‹è¯• {i}/{len(test_queries)}ã€‘{test_case['name']}")
            print("-" * 60)

            # å¿«é€Ÿæ¨¡å¼æµ‹è¯•
            print("âš¡ å¿«é€Ÿæ¨¡å¼æµ‹è¯•...")
            fast_start = time.time()
            fast_result = sql_optimizer.optimize_sql(test_case['sql'])
            fast_time = time.time() - fast_start

            print(f"   â±ï¸  å¿«é€Ÿæ¨¡å¼è€—æ—¶: {fast_time:.3f}s")
            print(f"   ğŸ” å‘ç°é—®é¢˜: {len(fast_result.get('issues_found', []))} ä¸ª")
            print(f"   âš¡ é¢„æœŸæå‡: {fast_result.get('performance_gain_estimate', 'N/A')}")

            # é‡å¤æŸ¥è¯¢æµ‹è¯•ç¼“å­˜æ•ˆæœ
            print("ğŸ”„ ç¼“å­˜æ•ˆæœæµ‹è¯•...")
            cache_start = time.time()
            cache_result = sql_optimizer.optimize_sql(test_case['sql'])
            cache_time = time.time() - cache_start

            print(f"   â±ï¸  ç¼“å­˜å‘½ä¸­è€—æ—¶: {cache_time:.3f}s")
            print(f"   ğŸ“ˆ ç¼“å­˜åŠ é€Ÿæ¯”: {fast_time/max(cache_time, 0.001):.1f}x")

        total_time = time.time() - total_start_time

        # æ˜¾ç¤ºæ€»ä½“æ€§èƒ½ç»Ÿè®¡
        print("\n" + "="*80)
        print("ğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“")
        print("="*80)

        stats = sql_optimizer.get_performance_stats()
        cache_stats = sql_analyzer.get_cache_stats()

        print(f"\nğŸ¯ ä¼˜åŒ–å™¨ç»Ÿè®¡:")
        print(f"   â€¢ æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"   â€¢ å¿«é€Ÿæ¨¡å¼ä½¿ç”¨: {stats['fast_mode_ratio']}")
        print(f"   â€¢ LLMæ¨¡å¼ä½¿ç”¨: {stats['llm_mode_ratio']}")
        print(f"   â€¢ å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.3f}s")
        print(f"   â€¢ æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']:.3f}s")

        print(f"\nğŸ’¾ ç¼“å­˜ç»Ÿè®¡:")
        print(f"   â€¢ ç¼“å­˜å‘½ä¸­: {cache_stats['hit_count']}")
        print(f"   â€¢ ç¼“å­˜æœªå‘½ä¸­: {cache_stats['miss_count']}")
        print(f"   â€¢ å‘½ä¸­ç‡: {cache_stats['hit_rate']}")
        print(f"   â€¢ ç¼“å­˜å¤§å°: {cache_stats['cache_size']}")

        print(f"\nâš¡ æ€§èƒ½æå‡:")
        print(f"   â€¢ æ€»æµ‹è¯•æ—¶é—´: {total_time:.3f}s")
        print(f"   â€¢ å¹³å‡æ¯æŸ¥è¯¢: {total_time/len(test_queries):.3f}s")

        # è¯¦ç»†æŠ¥å‘Šç¤ºä¾‹
        if len(test_queries) > 0:
            print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šç¤ºä¾‹ ({test_queries[0]['name']}):")
            print_simple_report(fast_result)

        print("\n" + "="*80)
        print("ğŸ‰ é«˜æ€§èƒ½ SQL ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def print_simple_report(result: Dict[str, Any]):
    """æ‰“å°ä¼˜åŒ–ç‰ˆæŠ¥å‘Š - åŒ…å«æ€§èƒ½ä¿¡æ¯"""

    print("\n" + "="*80)
    print("                é«˜æ€§èƒ½ SQL ä¼˜åŒ–æŠ¥å‘Š v2.0")
    print("="*80)

    print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"   å¤„ç†æ—¶é—´: {result.get('timestamp', 'N/A')}")
    print(f"   å¤„ç†æ¨¡å¼: {result.get('processing_mode', 'N/A')}")
    print(f"   å¤„ç†è€—æ—¶: {result.get('processing_time', 'N/A')}s")
    print(f"   Agent: {result.get('agent', 'fast_sql_optimizer')}")

    print(f"\nğŸ“ åŸå§‹ SQL:")
    print(result.get('original_sql', 'N/A'))

    print(f"\nâœ… ä¼˜åŒ–åçš„ SQL:")
    print(result.get('optimized_sql', 'N/A'))

    issues = result.get('issues_found', [])
    if issues:
        print(f"\nğŸ” å‘ç°çš„é—®é¢˜ ({len(issues)} ä¸ª):")
        for i, issue in enumerate(issues[:5], 1):
            print(f"   {i}. {issue}")
    else:
        print(f"\nâœ… æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜")

    optimizations = result.get('optimizations_applied', [])
    if optimizations:
        print(f"\nâš¡ åº”ç”¨çš„ä¼˜åŒ– ({len(optimizations)} ä¸ª):")
        for i, opt in enumerate(optimizations[:3], 1):
            print(f"   {i}. {opt}")

    print(f"\nğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡: {result.get('performance_gain_estimate', 'N/A')}")

    # æ€§èƒ½æŒ‡æ ‡
    metrics = result.get('analysis_metrics', {})
    if metrics:
        print(f"\nğŸ“Š åˆ†ææŒ‡æ ‡:")
        if metrics.get('joins', 0) > 0:
            print(f"   â€¢ JOIN æ•°é‡: {metrics['joins']}")
        if metrics.get('subqueries', 0) > 0:
            print(f"   â€¢ å­æŸ¥è¯¢æ•°é‡: {metrics['subqueries']}")
        if metrics.get('select_star', False):
            print(f"   â€¢ ä½¿ç”¨äº† SELECT *")
        if metrics.get('missing_where', False):
            print(f"   â€¢ ç¼ºå°‘ WHERE å­å¥")

    # ç¼“å­˜ç»Ÿè®¡
    cache_stats = result.get('cache_stats', {})
    if cache_stats:
        print(f"\nğŸ’¾ ç¼“å­˜ä¿¡æ¯:")
        print(f"   â€¢ å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 'N/A')}")
        print(f"   â€¢ ç¼“å­˜å¤§å°: {cache_stats.get('cache_size', 'N/A')}")

    recommendations = result.get('recommendations', [])
    if recommendations:
        print(f"\nğŸ’¡ é¢å¤–å»ºè®®:")
        for i, rec in enumerate(recommendations[:3], 1):
            print(f"   {i}. {rec}")

    print("\n" + "="*80)
    print("ğŸ‰ é«˜æ€§èƒ½ SQL ä¼˜åŒ–å®Œæˆ!")
    print("="*80)


if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              é«˜æ€§èƒ½ SQL ä¼˜åŒ–ç³»ç»Ÿ v2.0                          â•‘
    â•‘            (å¿«é€Ÿåˆ†æå¼•æ“ + CrewAI æ·±åº¦åˆ†æ)                    â•‘
    â•‘                                                                  â•‘
    â•‘  æŠ€æœ¯æ ˆ:                                                          â•‘
    â•‘    â€¢ é«˜æ€§èƒ½åˆ†æå¼•æ“    - å¹¶è¡Œæ¨¡å¼åˆ†æ + LRUç¼“å­˜                 â•‘
    â•‘    â€¢ CrewAI            - å•ä¸€ç»¼åˆ AI Agent (SQL ä¸“å®¶)           â•‘
    â•‘    â€¢ Ollama             - æœ¬åœ° LLM æœåŠ¡                          â•‘
    â•‘    â€¢ ThreadPoolExecutor - å¹¶å‘å¤„ç†èƒ½åŠ›                         â•‘
    â•‘                                                                  â•‘
    â•‘  æ€§èƒ½æ”¹è¿› v2.0:                                                  â•‘
    â•‘    â€¢ âš¡ å¿«é€Ÿæ¨¡å¼: <0.1s æœ¬åœ°åˆ†æï¼Œæ— éœ€LLMè°ƒç”¨                    â•‘
    â•‘    â€¢ ğŸ”„ æ™ºèƒ½ç¼“å­˜: é‡å¤æŸ¥è¯¢åŠ é€Ÿæ¯” 10-100x                        â•‘
    â•‘    â€¢ ğŸš€ å¹¶è¡Œåˆ†æ: å¤šæ¨¡å¼åŒæ—¶æ£€æµ‹ï¼Œæå‡åˆ†ææ•ˆç‡                   â•‘
    â•‘    â€¢ ğŸ“Š æ€§èƒ½ç›‘æ§: è¯¦ç»†çš„å¤„ç†æ—¶é—´å’Œç¼“å­˜ç»Ÿè®¡                        â•‘
    â•‘    â€¢ ğŸ›¡ï¸ å®¹é”™æœºåˆ¶: LLMå¤±è´¥è‡ªåŠ¨é™çº§åˆ°å¿«é€Ÿæ¨¡å¼                      â•‘
    â•‘                                                                  â•‘
    â•‘  å·¥ä½œæµ:                                                         â•‘
    â•‘    ç”¨æˆ· SQL â†’ æ¨¡å¼åŒ¹é… â†’ ç¼“å­˜æŸ¥è¯¢ â†’ å¿«é€Ÿä¼˜åŒ–/LLMæ·±åº¦åˆ†æ â†’ æŠ¥å‘Š    â•‘
    â•‘                                                                  â•‘
    â•‘  æ€§èƒ½æå‡:                                                       â•‘
    â•‘    â€¢ åˆ†æé€Ÿåº¦: æå‡ 10-50x (å¿«é€Ÿæ¨¡å¼)                           â•‘
    â•‘    â€¢ ç¼“å­˜å‘½ä¸­: åŠ é€Ÿ 10-100x (é‡å¤æŸ¥è¯¢)                          â•‘
    â•‘    â€¢ å†…å­˜ä½¿ç”¨: ä¼˜åŒ– <50MB                                       â•‘
    â•‘    â€¢ å¹¶å‘èƒ½åŠ›: æ”¯æŒ 4 çº¿ç¨‹å¹¶è¡Œåˆ†æ                              â•‘
    â•‘                                                                  â•‘
    â•‘  ä¾èµ–å®‰è£…:                                                       â•‘
    â•‘    pip install crewai crewai-tools                              â•‘
    â•‘    pip install python-dotenv                                     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    main()
